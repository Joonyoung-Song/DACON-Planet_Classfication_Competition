{
 "cells": [
  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import skew, kurtosis\n",
    "import itertools\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from pathlib import Path\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = pd.read_csv('C:/python/DACON_DATA/train.csv', index_col=0)\n",
    "tst = pd.read_csv('C:/python/DACON_DATA/test.csv', index_col=0)\n",
    "\n",
    "all_fea=['u','g','r','i','z','redshift','dered_u','dered_g','dered_r','dered_i','dered_z','nObserve','nDetect','airmass_u','airmass_g','airmass_r','airmass_i','airmass_z']\n",
    "fea=['u', 'g', 'r', 'i', 'z', 'redshift', 'dered_u', 'dered_g','dered_r', 'dered_i', 'dered_z']\n",
    "fea2=['u','g','r','i','z','redshift','dered_u','dered_g','dered_r','dered_i','dered_z','nObserve','nDetect']\n",
    "names=['u','g','r','i','z']\n",
    "names_2 = ['dered_u','dered_g','dered_r','dered_i','dered_z']\n",
    "airmass=['airmass_u','airmass_g','airmass_r','airmass_i','airmass_z']\n",
    "\n",
    "#이상치 제거\n",
    "for i in range(len(fea)):\n",
    "    trn=trn[trn[fea[i]]>np.min(tst[fea[i]], axis=0)]\n",
    "    trn=trn[trn[fea[i]]<np.max(tst[fea[i]], axis=0)]\n",
    "    \n",
    "# 설명변수와 반응변수 분리\n",
    "trn_target = trn['class']\n",
    "trn = trn.drop('class', axis=1)\n",
    "\n",
    "# 옵저브 디텍트 연속형으로 전환\n",
    "trn['nObserve']=trn['nObserve'].astype('float')\n",
    "trn['nDetect']=trn['nDetect'].astype('float')\n",
    "trnnO = trn['nObserve']\n",
    "trnnD = trn['nDetect']\n",
    "\n",
    "#카테고리별 max, min, max-min, std, sum을 구한다.\n",
    "#max-min\n",
    "trn['max-min'] = trn[all_fea].max(axis=1)-trn[all_fea].min(axis=1)\n",
    "trn['max-min_ugriz'] = trn[names].max(axis=1)-trn[names].min(axis=1)\n",
    "trn['max-min_dered'] = trn[names_2].max(axis=1)-trn[names_2].min(axis=1)\n",
    "#std\n",
    "trn['std'] = trn[all_fea].std(axis=1)\n",
    "trn['std_ugriz'] = trn[names].std(axis=1)\n",
    "trn['std_dered'] = trn[names_2].std(axis=1)\n",
    "#파장별 합\n",
    "trn['sum'] = trn[all_fea].sum(axis=1)\n",
    "trn['sum_ugriz'] = trn[names].sum(axis=1)\n",
    "trn['sum_dered'] = trn[names_2].sum(axis=1)\n",
    "#파장별 최대값\n",
    "trn['max'] = trn[all_fea].max(axis=1)\n",
    "trn['max_ugriz'] = trn[names].max(axis=1)\n",
    "trn['max_dered'] = trn[names_2].max(axis=1)\n",
    "#파장별 최소값\n",
    "trn['min'] = trn[all_fea].min(axis=1)\n",
    "trn['min_ugriz'] = trn[names].min(axis=1)\n",
    "trn['min_dered'] = trn[names_2].min(axis=1)\n",
    "#파장별 max-max,min=min,sum-sum\n",
    "trn['max-max']=trn[names].max(axis=1)-trn[names_2].max(axis=1)\n",
    "trn['min-min']=trn[names].min(axis=1)-trn[names_2].min(axis=1)\n",
    "trn['sum-sum']=trn[names].sum(axis=1)-trn[names_2].sum(axis=1)\n",
    "\n",
    "#왜도,첨도 구하기\n",
    "trn['skew']=skew(trn[names],axis=1)\n",
    "trn['kurtosis']=kurtosis(trn[names],axis=1)\n",
    "trn['dered_skew']=skew(trn[names_2],axis=1)\n",
    "trn['dered_kurtosis']=kurtosis(trn[names_2],axis=1)\n",
    "trn['airmass_skew']=skew(trn[airmass],axis=1)\n",
    "trn['airmass_kurtosis']=kurtosis(trn[airmass],axis=1)\n",
    "\n",
    "\n",
    "#조합으로 연산 피쳐 생성\n",
    "for c1,c2 in tqdm(itertools.combinations(fea2,2)):\n",
    "    dif_col=f'diff_{c1}_{c2}'\n",
    "    div_col=f'div_{c1}_{c2}'\n",
    "    sum_col=f'sum_{c1}_{c2}'\n",
    "    mul_col=f'mul_{c1}_{c2}'\n",
    "    trn[dif_col]=trn[c1]-trn[c2]\n",
    "    trn[div_col]=trn[c1]/trn[c2]\n",
    "    trn[sum_col]=trn[c1]+trn[c2]\n",
    "    trn[mul_col]=trn[c1]*trn[c2]\n",
    "\n",
    "\n",
    "# 소수점 4자리 까지만 나타내는 asinh 변수 생성\n",
    "trn['asinh_mu'] = -2.5/np.log(10)*(np.arcsinh(trn.u/24.63/(2.8e-10))-22.689378693319245)\n",
    "trn['asinh_mg'] = -2.5/np.log(10)*(np.arcsinh(trn.g/25.11/(1.8e-10))-23.131211445598282)\n",
    "trn['asinh_mr'] = -2.5/np.log(10)*(np.arcsinh(trn.r/24.80/(2.4e-10))-22.843529373146502)\n",
    "trn['asinh_mi'] = -2.5/np.log(10)*(np.arcsinh(trn.i/24.36/(3.6e-10))-22.43806426503834)\n",
    "trn['asinh_mz'] = -2.5/np.log(10)*(np.arcsinh(trn.z/22.83/(1.48e-09))-21.024370929730330)\n",
    "\n",
    "\n",
    "trn['redshift%14'] = trn['redshift']%14\n",
    "trn['log_redshift']=np.log1p(trn['redshift'])\n",
    "trn['log_redshift']=trn['log_redshift'].fillna(0)\n",
    "\n",
    "#도메인에서 얻은 파생변수 생성\n",
    "#출처: https://www.sdss.org/dr16/algorithms/segue_target_selection/#Legacy\n",
    "trn['l-color'] = (-0.436*trn['u']) + (1.129*trn['g']) - (0.119*trn['r']) - (0.574*trn['i']) + (0.1984)\n",
    "trn['s-color'] = (-0.249*trn['u']) + (0.794*trn['g']) - (0.555*trn['r']) + (0.234)\n",
    "trn['P1'] = (0.91*(trn['u']-trn['g'])) + (0.415*(trn['g']-trn['r'])) - (1.280)\n",
    "\n",
    "trn['class'] = trn_target\n",
    "\n",
    "#피쳐 제거\n",
    "PI30_1=['dered_z','dered_g','div_dered_g_dered_z','sum_dered_i_nDetect','mul_dered_g_dered_z','sum_z_dered_i','diff_r_redshift','sum_u_redshift','sum_dered_r_nDetect','mul_g_dered_z','sum_dered_r_dered_z','div_r_nObserve','sum_i_redshift','diff_dered_g_dered_z','sum_dered_g_dered_r','sum_r_dered_z','diff_u_redshift','mul_u_dered_z','mul_dered_g_dered_r','mul_i_dered_r','div_i_dered_g','sum_r_redshift','div_u_dered_z','mul_r_z','div_g_z','diff_u_dered_z','mul_r_dered_g','sum_redshift_dered_z','u','div_redshift_dered_u']\n",
    "trn=trn[trn.columns.difference(PI30_1)]\n",
    "PI30_2=['mul_dered_g_nDetect','sum_r_nObserve','sum_i_dered_r','diff_z_nObserve','mul_nObserve_nDetect','asinh_mz','nObserve','asinh_mg','diff_g_dered_z','mul_g_dered_u','log_redshift','nDetect','mul_u_z','sum_r_nDetect','div_dered_i_nObserve','sum_i_z','sum_u_dered_r','dered_i','mul_g_dered_r','mul_z_dered_i','div_g_nDetect','diff_dered_g_nObserve','mul_r_dered_z','sum_i_nDetect','diff_z_dered_g','mul_g_z','sum_z_dered_z','mul_dered_u_nObserve','div_redshift_nObserve','dered_r']\n",
    "trn=trn[trn.columns.difference(PI30_2)]\n",
    "PI30_3=[\"mul_g_i\",\"sum_redshift_dered_u\",\"sum_g_i\",\"sum_z_dered_g\",\"sum_i_dered_z\",\"mul_r_dered_r\",\"div_g_i\",\"mul_dered_r_dered_i\",\"g\",\"div_dered_i_nDetect\",\"mul_dered_r_nObserve\",\"sum_r_i\",\"min_dered\",\"mul_i_dered_z\",\"div_dered_r_nObserve\",\"diff_dered_z_nDetect\",\"div_i_nDetect\",\"sum_nObserve_nDetect\",\"max_dered\",\"mul_g_r\",\"div_z_nObserve\",\"sum_i_dered_u\",\"mul_dered_r_nDetect\",\"div_dered_g_nObserve\",\"mul_r_i\",\"diff_i_nDetect\",\"sum_u_r\",\"sum_dered_g_nDetect\",\"div_u_nDetect\",\"sum_u_dered_g\"]\n",
    "trn=trn[trn.columns.difference(PI30_3)]\n",
    "PI30_4=[\"div_r_nDetect\",\"mul_u_g\",\"diff_dered_i_nDetect\",\"mul_u_i\",\"mul_dered_i_nObserve\",\"div_i_dered_i\",\"mul_i_nDetect\",\"mul_g_dered_g\",\"sum_u_dered_z\",\"mul_r_dered_i\",\"sum_g_dered_g\",\"sum_u_g\",\"mul_dered_u_dered_i\",\"sum_g_nDetect\",\"mul_z_nObserve\",\"mul_g_nDetect\",\"mul_dered_i_nDetect\",\"sum_dered_g_dered_i\",\"mul_u_dered_i\",\"div_dered_z_nDetect\",\"i\",\"sum_g_z\",\"sum_u_dered_u\",\"sum_g_r\",\"sum_dered\",\"mul_u_nDetect\",\"mul_i_dered_i\",\"mul_dered_g_nObserve\",\"div_dered_g_nDetect\",\"div_u_i\"]\n",
    "trn=trn[trn.columns.difference(PI30_4)]\n",
    "PI30_5=[\"asinh_mr\",\"div_dered_r_nDetect\",\"mul_z_dered_r\",\"mul_g_nObserve\",\"diff_u_nDetect\",\"sum_g_dered_u\",\"sum_redshift_dered_i\",\"div_i_nObserve\",\"div_g_nObserve\",\"z\",\"mul_dered_z_nObserve\",\"sum_dered_r_nObserve\",\"sum_z_dered_r\",\"sum_u_dered_i\",\"mul_u_dered_g\",\"mul_dered_u_dered_r\",\"diff_z_nDetect\",\"sum_r_dered_r\",\"sum_dered_u_dered_g\",\"asinh_mi\",\"diff_i_redshift\",\"diff_r_nObserve\",\"mul_i_nObserve\",\"diff_dered_r_nObserve\",\"sum_i_dered_i\",\"sum_r_dered_i\",\"diff_dered_r_nDetect\",\"sum_u_nObserve\",\"div_dered_u_nDetect\",\"sum_z_dered_u\"]\n",
    "trn=trn[trn.columns.difference(PI30_5)]\n",
    "PI30_6=[\"sum_dered_i_nObserve\",\"diff_nObserve_nDetect\",\"mul_dered_u_dered_z\",\"div_u_nObserve\",\"diff_u_nObserve\",\"diff_redshift_nDetect\",\"mul_u_nObserve\",\"diff_dered_g_nDetect\",\"sum_u_z\",\"max_ugriz\",\"sum_g_dered_r\",\"mul_r_nObserve\",\"div_z_nDetect\",\"max\",\"mul_dered_u_nDetect\",\"sum_i_nObserve\",\"diff_g_nDetect\",\"sum_dered_z_nDetect\",\"mul_z_nDetect\",\"mul_i_z\",\"diff_r_nDetect\",\"diff_g_nObserve\",\"div_g_dered_i\",\"mul_dered_z_nDetect\",\"mul_i_dered_u\",\"diff_dered_u_nObserve\",\"div_dered_z_nObserve\",\"mul_u_r\",\"diff_i_nObserve\",\"sum_ugriz\"]\n",
    "trn=trn[trn.columns.difference(PI30_6)]\n",
    "PI30_7=[\"dered_u\",\"mul_i_dered_g\",\"mul_r_dered_u\",\"mul_r_nDetect\",\"sum_dered_g_nObserve\",\"sum-sum\",\"diff_dered_u_dered_z\",\"diff_dered_u_nDetect\",\"mul_dered_r_dered_z\",\"sum_dered_u_nObserve\",\"diff_dered_i_nObserve\",\"diff_g_z\",\"diff_z_redshift\",\"diff_i_dered_g\",\"mul_z_dered_z\",\"min_ugriz\",\"diff_dered_z_nObserve\",\"mul_g_dered_i\",\"asinh_mu\",\"sum_dered_u_dered_r\",\"diff_u_z\",\"sum_u_nDetect\",\"std_ugriz\",\"sum_dered_r_dered_i\",\"sum_dered_z_nObserve\",\"mul_dered_u_dered_g\",\"sum_r_dered_g\",\"mul_u_dered_r\",\"max-min_ugriz\",\"diff_redshift_dered_r\"]\n",
    "trn=trn[trn.columns.difference(PI30_7)]\n",
    "PI30_8=[\"div_i_redshift\",\"sum_g_redshift\",\"div_r_dered_r\",\"mul_z_dered_g\",\"sum_redshift_nDetect\",\"diff_u_dered_u\",\"diff_i_dered_i\",\"diff_i_dered_u\",\"sum_g_dered_i\",\"diff_u_dered_i\",\"div_g_redshift\",\"sum_u_i\",\"div_z_dered_r\",\"sum_i_dered_g\",\"sum_redshift_dered_r\",\"sum_z_nObserve\",\"sum_dered_u_nDetect\",\"diff_redshift_nObserve\",\"sum_dered_u_dered_z\",\"div_g_dered_z\",\"sum_z_nDetect\",\"diff_redshift_dered_z\",\"div_dered_g_dered_i\",\"div_dered_u_nObserve\",\"mul_u_dered_u\",\"sum_r_z\",\"r\",\"max-min_dered\",\"mul_dered_g_dered_i\",\"sum_g_nObserve\"]\n",
    "trn=trn[trn.columns.difference(PI30_8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fea=['u','g','r','i','z','redshift','dered_u','dered_g','dered_r','dered_i','dered_z','nObserve','nDetect','airmass_u','airmass_g','airmass_r','airmass_i','airmass_z']\n",
    "fea=['u', 'g', 'r', 'i', 'z', 'redshift', 'dered_u', 'dered_g','dered_r', 'dered_i', 'dered_z']\n",
    "fea2=['u','g','r','i','z','redshift','dered_u','dered_g','dered_r','dered_i','dered_z','nObserve','nDetect']\n",
    "names=['u','g','r','i','z']\n",
    "names_2 = ['dered_u','dered_g','dered_r','dered_i','dered_z']\n",
    "airmass=['airmass_u','airmass_g','airmass_r','airmass_i','airmass_z']\n",
    "\n",
    "# #이상치 제거\n",
    "# for i in range(len(fea)):\n",
    "#     tst=tst[tst[fea[i]]>np.min(tst[fea[i]], axis=0)]\n",
    "#     tst=tst[tst[fea[i]]<np.max(tst[fea[i]], axis=0)]\n",
    "    \n",
    "# # 설명변수와 반응변수 분리\n",
    "# tst_target = tst['class']\n",
    "# tst = tst.drop('class', axis=1)\n",
    "\n",
    "# 옵저브 디텍트 연속형으로 전환\n",
    "tst['nObserve']=tst['nObserve'].astype('float')\n",
    "tst['nDetect']=tst['nDetect'].astype('float')\n",
    "tstnO = tst['nObserve']\n",
    "tstnD = tst['nDetect']\n",
    "\n",
    "#카테고리별 max, min, max-min, std, sum을 구한다.\n",
    "#max-min\n",
    "tst['max-min'] = tst[all_fea].max(axis=1)-tst[all_fea].min(axis=1)\n",
    "tst['max-min_ugriz'] = tst[names].max(axis=1)-tst[names].min(axis=1)\n",
    "tst['max-min_dered'] = tst[names_2].max(axis=1)-tst[names_2].min(axis=1)\n",
    "#std\n",
    "tst['std'] = tst[all_fea].std(axis=1)\n",
    "tst['std_ugriz'] = tst[names].std(axis=1)\n",
    "tst['std_dered'] = tst[names_2].std(axis=1)\n",
    "#파장별 합\n",
    "tst['sum'] = tst[all_fea].sum(axis=1)\n",
    "tst['sum_ugriz'] = tst[names].sum(axis=1)\n",
    "tst['sum_dered'] = tst[names_2].sum(axis=1)\n",
    "#파장별 최대값\n",
    "tst['max'] = tst[all_fea].max(axis=1)\n",
    "tst['max_ugriz'] = tst[names].max(axis=1)\n",
    "tst['max_dered'] = tst[names_2].max(axis=1)\n",
    "#파장별 최소값\n",
    "tst['min'] = tst[all_fea].min(axis=1)\n",
    "tst['min_ugriz'] = tst[names].min(axis=1)\n",
    "tst['min_dered'] = tst[names_2].min(axis=1)\n",
    "#파장별 max-max,min=min,sum-sum\n",
    "tst['max-max']=tst[names].max(axis=1)-tst[names_2].max(axis=1)\n",
    "tst['min-min']=tst[names].min(axis=1)-tst[names_2].min(axis=1)\n",
    "tst['sum-sum']=tst[names].sum(axis=1)-tst[names_2].sum(axis=1)\n",
    "\n",
    "#왜도,첨도 구하기\n",
    "tst['skew']=skew(tst[names],axis=1)\n",
    "tst['kurtosis']=kurtosis(tst[names],axis=1)\n",
    "tst['dered_skew']=skew(tst[names_2],axis=1)\n",
    "tst['dered_kurtosis']=kurtosis(tst[names_2],axis=1)\n",
    "tst['airmass_skew']=skew(tst[airmass],axis=1)\n",
    "tst['airmass_kurtosis']=kurtosis(tst[airmass],axis=1)\n",
    "\n",
    "\n",
    "#조합으로 연산 피쳐 생성\n",
    "for c1,c2 in tqdm(itertools.combinations(fea2,2)):\n",
    "    dif_col=f'diff_{c1}_{c2}'\n",
    "    div_col=f'div_{c1}_{c2}'\n",
    "    sum_col=f'sum_{c1}_{c2}'\n",
    "    mul_col=f'mul_{c1}_{c2}'\n",
    "    tst[dif_col]=tst[c1]-tst[c2]\n",
    "    tst[div_col]=tst[c1]/tst[c2]\n",
    "    tst[sum_col]=tst[c1]+tst[c2]\n",
    "    tst[mul_col]=tst[c1]*tst[c2]\n",
    "\n",
    "\n",
    "# 소수점 4자리 까지만 나타내는 asinh 변수 생성\n",
    "tst['asinh_mu'] = -2.5/np.log(10)*(np.arcsinh(tst.u/24.63/(2.8e-10))-22.689378693319245)\n",
    "tst['asinh_mg'] = -2.5/np.log(10)*(np.arcsinh(tst.g/25.11/(1.8e-10))-23.131211445598282)\n",
    "tst['asinh_mr'] = -2.5/np.log(10)*(np.arcsinh(tst.r/24.80/(2.4e-10))-22.843529373146502)\n",
    "tst['asinh_mi'] = -2.5/np.log(10)*(np.arcsinh(tst.i/24.36/(3.6e-10))-22.43806426503834)\n",
    "tst['asinh_mz'] = -2.5/np.log(10)*(np.arcsinh(tst.z/22.83/(1.48e-09))-21.024370929730330)\n",
    "\n",
    "\n",
    "tst['redshift%14'] = tst['redshift']%14\n",
    "tst['log_redshift']=np.log1p(tst['redshift'])\n",
    "tst['log_redshift']=tst['log_redshift'].fillna(0)\n",
    "\n",
    "#도메인에서 얻은 파생변수 생성\n",
    "#출처: https://www.sdss.org/dr16/algorithms/segue_target_selection/#Legacy\n",
    "tst['l-color'] = (-0.436*tst['u']) + (1.129*tst['g']) - (0.119*tst['r']) - (0.574*tst['i']) + (0.1984)\n",
    "tst['s-color'] = (-0.249*tst['u']) + (0.794*tst['g']) - (0.555*tst['r']) + (0.234)\n",
    "tst['P1'] = (0.91*(tst['u']-tst['g'])) + (0.415*(tst['g']-tst['r'])) - (1.280)\n",
    "\n",
    "\n",
    "# tst['class'] = tst_target\n",
    "\n",
    "#피쳐 제거\n",
    "PI30_1=['dered_z','dered_g','div_dered_g_dered_z','sum_dered_i_nDetect','mul_dered_g_dered_z','sum_z_dered_i','diff_r_redshift','sum_u_redshift','sum_dered_r_nDetect','mul_g_dered_z','sum_dered_r_dered_z','div_r_nObserve','sum_i_redshift','diff_dered_g_dered_z','sum_dered_g_dered_r','sum_r_dered_z','diff_u_redshift','mul_u_dered_z','mul_dered_g_dered_r','mul_i_dered_r','div_i_dered_g','sum_r_redshift','div_u_dered_z','mul_r_z','div_g_z','diff_u_dered_z','mul_r_dered_g','sum_redshift_dered_z','u','div_redshift_dered_u']\n",
    "tst=tst[tst.columns.difference(PI30_1)]\n",
    "PI30_2=['mul_dered_g_nDetect','sum_r_nObserve','sum_i_dered_r','diff_z_nObserve','mul_nObserve_nDetect','asinh_mz','nObserve','asinh_mg','diff_g_dered_z','mul_g_dered_u','log_redshift','nDetect','mul_u_z','sum_r_nDetect','div_dered_i_nObserve','sum_i_z','sum_u_dered_r','dered_i','mul_g_dered_r','mul_z_dered_i','div_g_nDetect','diff_dered_g_nObserve','mul_r_dered_z','sum_i_nDetect','diff_z_dered_g','mul_g_z','sum_z_dered_z','mul_dered_u_nObserve','div_redshift_nObserve','dered_r']\n",
    "tst=tst[tst.columns.difference(PI30_2)]\n",
    "PI30_3=[\"mul_g_i\",\"sum_redshift_dered_u\",\"sum_g_i\",\"sum_z_dered_g\",\"sum_i_dered_z\",\"mul_r_dered_r\",\"div_g_i\",\"mul_dered_r_dered_i\",\"g\",\"div_dered_i_nDetect\",\"mul_dered_r_nObserve\",\"sum_r_i\",\"min_dered\",\"mul_i_dered_z\",\"div_dered_r_nObserve\",\"diff_dered_z_nDetect\",\"div_i_nDetect\",\"sum_nObserve_nDetect\",\"max_dered\",\"mul_g_r\",\"div_z_nObserve\",\"sum_i_dered_u\",\"mul_dered_r_nDetect\",\"div_dered_g_nObserve\",\"mul_r_i\",\"diff_i_nDetect\",\"sum_u_r\",\"sum_dered_g_nDetect\",\"div_u_nDetect\",\"sum_u_dered_g\"]\n",
    "tst=tst[tst.columns.difference(PI30_3)]\n",
    "PI30_4=[\"div_r_nDetect\",\"mul_u_g\",\"diff_dered_i_nDetect\",\"mul_u_i\",\"mul_dered_i_nObserve\",\"div_i_dered_i\",\"mul_i_nDetect\",\"mul_g_dered_g\",\"sum_u_dered_z\",\"mul_r_dered_i\",\"sum_g_dered_g\",\"sum_u_g\",\"mul_dered_u_dered_i\",\"sum_g_nDetect\",\"mul_z_nObserve\",\"mul_g_nDetect\",\"mul_dered_i_nDetect\",\"sum_dered_g_dered_i\",\"mul_u_dered_i\",\"div_dered_z_nDetect\",\"i\",\"sum_g_z\",\"sum_u_dered_u\",\"sum_g_r\",\"sum_dered\",\"mul_u_nDetect\",\"mul_i_dered_i\",\"mul_dered_g_nObserve\",\"div_dered_g_nDetect\",\"div_u_i\"]\n",
    "tst=tst[tst.columns.difference(PI30_4)]\n",
    "PI30_5=[\"asinh_mr\",\"div_dered_r_nDetect\",\"mul_z_dered_r\",\"mul_g_nObserve\",\"diff_u_nDetect\",\"sum_g_dered_u\",\"sum_redshift_dered_i\",\"div_i_nObserve\",\"div_g_nObserve\",\"z\",\"mul_dered_z_nObserve\",\"sum_dered_r_nObserve\",\"sum_z_dered_r\",\"sum_u_dered_i\",\"mul_u_dered_g\",\"mul_dered_u_dered_r\",\"diff_z_nDetect\",\"sum_r_dered_r\",\"sum_dered_u_dered_g\",\"asinh_mi\",\"diff_i_redshift\",\"diff_r_nObserve\",\"mul_i_nObserve\",\"diff_dered_r_nObserve\",\"sum_i_dered_i\",\"sum_r_dered_i\",\"diff_dered_r_nDetect\",\"sum_u_nObserve\",\"div_dered_u_nDetect\",\"sum_z_dered_u\"]\n",
    "tst=tst[tst.columns.difference(PI30_5)]\n",
    "PI30_6=[\"sum_dered_i_nObserve\",\"diff_nObserve_nDetect\",\"mul_dered_u_dered_z\",\"div_u_nObserve\",\"diff_u_nObserve\",\"diff_redshift_nDetect\",\"mul_u_nObserve\",\"diff_dered_g_nDetect\",\"sum_u_z\",\"max_ugriz\",\"sum_g_dered_r\",\"mul_r_nObserve\",\"div_z_nDetect\",\"max\",\"mul_dered_u_nDetect\",\"sum_i_nObserve\",\"diff_g_nDetect\",\"sum_dered_z_nDetect\",\"mul_z_nDetect\",\"mul_i_z\",\"diff_r_nDetect\",\"diff_g_nObserve\",\"div_g_dered_i\",\"mul_dered_z_nDetect\",\"mul_i_dered_u\",\"diff_dered_u_nObserve\",\"div_dered_z_nObserve\",\"mul_u_r\",\"diff_i_nObserve\",\"sum_ugriz\"]\n",
    "tst=tst[tst.columns.difference(PI30_6)]\n",
    "PI30_7=[\"dered_u\",\"mul_i_dered_g\",\"mul_r_dered_u\",\"mul_r_nDetect\",\"sum_dered_g_nObserve\",\"sum-sum\",\"diff_dered_u_dered_z\",\"diff_dered_u_nDetect\",\"mul_dered_r_dered_z\",\"sum_dered_u_nObserve\",\"diff_dered_i_nObserve\",\"diff_g_z\",\"diff_z_redshift\",\"diff_i_dered_g\",\"mul_z_dered_z\",\"min_ugriz\",\"diff_dered_z_nObserve\",\"mul_g_dered_i\",\"asinh_mu\",\"sum_dered_u_dered_r\",\"diff_u_z\",\"sum_u_nDetect\",\"std_ugriz\",\"sum_dered_r_dered_i\",\"sum_dered_z_nObserve\",\"mul_dered_u_dered_g\",\"sum_r_dered_g\",\"mul_u_dered_r\",\"max-min_ugriz\",\"diff_redshift_dered_r\"]\n",
    "tst=tst[tst.columns.difference(PI30_7)]\n",
    "PI30_8=[\"div_i_redshift\",\"sum_g_redshift\",\"div_r_dered_r\",\"mul_z_dered_g\",\"sum_redshift_nDetect\",\"diff_u_dered_u\",\"diff_i_dered_i\",\"diff_i_dered_u\",\"sum_g_dered_i\",\"diff_u_dered_i\",\"div_g_redshift\",\"sum_u_i\",\"div_z_dered_r\",\"sum_i_dered_g\",\"sum_redshift_dered_r\",\"sum_z_nObserve\",\"sum_dered_u_nDetect\",\"diff_redshift_nObserve\",\"sum_dered_u_dered_z\",\"div_g_dered_z\",\"sum_z_nDetect\",\"diff_redshift_dered_z\",\"div_dered_g_dered_i\",\"div_dered_u_nObserve\",\"mul_u_dered_u\",\"sum_r_z\",\"r\",\"max-min_dered\",\"mul_dered_g_dered_i\",\"sum_g_nObserve\"]\n",
    "tst=tst[tst.columns.difference(PI30_8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base learner modeling\n",
    "- 1) Light GBM\n",
    "- 2) XGBoost\n",
    "- 3) Random Forest\n",
    "- 4) Multi Layer Perceptron\n",
    "- 5) Naive Bayes\n",
    "- 6) Support Vector Machine\n",
    "- 7) Hist GBM\n",
    "- 8) Cat Boost\n",
    "- 9) GBM\n",
    "- 10) Xtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "n_class = 3\n",
    "seed = 42\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr=trn.drop(\"class\",axis=1)\n",
    "target=trn['class']\n",
    "ftr=ftr.values\n",
    "tst_ar=tst.values\n",
    "target=target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_p_val = np.zeros((ftr.shape[0], n_class))\n",
    "lgb_p_tst = np.zeros((tst_ar.shape[0], n_class))\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(ftr, target), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    lgb_clf = lgb.LGBMClassifier(objective='multiclass',\n",
    "                             n_estimators=600,\n",
    "                             boosting_type ='dart',\n",
    "                             num_leaves=100,\n",
    "                             learning_rate=0.1,\n",
    "                             max_depth = 30,\n",
    "                              feature_fraction=0.8,\n",
    "                             random_state=seed,                             \n",
    "                             n_jobs=-1)\n",
    "    lgb_clf.fit(ftr[i_trn], target[i_trn],\n",
    "            eval_set=[(ftr[i_val], target[i_val])],\n",
    "            eval_metric='multi_error')\n",
    "    \n",
    "    lgb_p_val[i_val, :] = lgb_clf.predict_proba(ftr[i_val])\n",
    "    lgb_p_tst += lgb_clf.predict_proba(tst_ar) / n_fold\n",
    "print(f'{accuracy_score(target, np.argmax(lgb_p_val, axis=1)) * 100:.4f}%')\n",
    "print(f'{confusion_matrix(target, np.argmax(lgb_p_val, axis=1))}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_dir = Path('C:/Users/ATIV/python/stacking/tst')\n",
    "val_dir = Path('C:/Users/ATIV/python/stacking/val')\n",
    "\n",
    "algo_name = 'lgb'\n",
    "feature_name = '124'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "p_val_file = val_dir / f'{model_name}.val.csv'\n",
    "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
    "\n",
    "np.savetxt(p_val_file, lgb_p_val, fmt='%.6f', delimiter=',')\n",
    "np.savetxt(p_tst_file, lgb_p_tst, fmt='%.6f', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr=trn.drop(\"class\",axis=1)\n",
    "target=trn['class']\n",
    "ftr=ftr.values\n",
    "tst_ar=tst.values\n",
    "target=target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_p_val = np.zeros((ftr.shape[0], n_class))\n",
    "xgb_p_tst = np.zeros((tst_ar.shape[0], n_class))\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(ftr, target), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    xgb_clf = xgb.XGBClassifier(learning_rate=0.1,\n",
    "                          n_estimators=220,\n",
    "                          max_depth=10,\n",
    "                           feature_fraction=0.8,\n",
    "                          booster='dart',\n",
    "                            random_state=seed,\n",
    "                          tree_method='exact',\n",
    "                            objective='multiclass',\n",
    "                            num_class=3,\n",
    "                            n_jobs=-1)\n",
    "    xgb_clf.fit(ftr[i_trn], target[i_trn],\n",
    "            eval_set=[(ftr[i_val], target[i_val])])\n",
    "    \n",
    "    xgb_p_val[i_val, :] = xgb_clf.predict_proba(ftr[i_val])\n",
    "    xgb_p_tst += xgb_clf.predict_proba(tst_ar) / n_fold\n",
    "print(f'{accuracy_score(target, np.argmax(xgb_p_val, axis=1)) * 100:.4f}%')\n",
    "print(f'{confusion_matrix(target, np.argmax(xgb_p_val, axis=1))}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_dir = Path('C:/Users/ATIV/python/stacking/tst')\n",
    "val_dir = Path('C:/Users/ATIV/python/stacking/val')\n",
    "\n",
    "algo_name = 'xgb'\n",
    "feature_name = '124'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "p_val_file = val_dir / f'{model_name}.val.csv'\n",
    "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
    "\n",
    "np.savetxt(p_val_file, xgb_p_val, fmt='%.6f', delimiter=',')\n",
    "np.savetxt(p_tst_file, xgb_p_tst, fmt='%.6f', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr=trn.drop(\"class\",axis=1)\n",
    "target=trn['class']\n",
    "ftr=ftr.values\n",
    "tst_ar=tst.values\n",
    "target=target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_p_val = np.zeros((ftr.shape[0], n_class))\n",
    "rf_p_tst = np.zeros((tst_ar.shape[0], n_class))\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(ftr, target), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    rf_clf = RandomForestClassifier(n_estimators = 200, \n",
    "                                random_state=seed,\n",
    "                                verbose=True,\n",
    "                                oob_score=True,\n",
    "                                n_jobs=-1,\n",
    "                               max_depth=25)\n",
    "    rf_clf.fit(ftr[i_trn], target[i_trn])\n",
    "    rf_p_val[i_val, :] = rf_clf.predict_proba(ftr[i_val])\n",
    "    rf_p_tst += rf_clf.predict_proba(tst_ar) / n_fold\n",
    "print(f'{accuracy_score(target, np.argmax(rf_p_val, axis=1)) * 100:.4f}%')\n",
    "print(f'{confusion_matrix(target, np.argmax(rf_p_val, axis=1))}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_dir = Path('C:/Users/ATIV/python/stacking/tst')\n",
    "val_dir = Path('C:/Users/ATIV/python/stacking/val')\n",
    "\n",
    "algo_name = 'rf'\n",
    "feature_name = '124'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "p_val_file = val_dir / f'{model_name}.val.csv'\n",
    "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
    "\n",
    "np.savetxt(p_val_file, rf_p_val, fmt='%.6f', delimiter=',')\n",
    "np.savetxt(p_tst_file, rf_p_tst, fmt='%.6f', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#상관계수 높은 변수 제거\n",
    "corr = trn.loc[trn['class']!=0,:].corr()\n",
    "cor_list=[]\n",
    "for i in range(0,trn.loc[trn['class']!=0,:].shape[1]):\n",
    "    for j in range(0,trn.loc[trn['class']!=0,:].shape[1]):\n",
    "        if abs(corr.iloc[i,j])>=0.9 and corr.index[i]!=corr.columns[j]:\n",
    "            cor_list += [corr.columns[j]]\n",
    "            cor_list += [corr.index[i]]\n",
    "groupby=cor_list\n",
    "cor_result=dict()\n",
    "for ip in tqdm(cor_list):\n",
    "    cor_result[ip]=cor_list.count(ip)\n",
    "final_corr= []\n",
    "for i in range(0,len(cor_result)):\n",
    "    if list(cor_result.values())[i] > 20 :\n",
    "        final_corr.append(list(cor_result.keys())[i])\n",
    "print(cor_result)\n",
    "print()\n",
    "print(final_corr)\n",
    "cols = list(trn.columns[~trn.columns.isin(list(cor_result.keys()))])\n",
    "for i in ['diff_u_dered_g','P1','diff_u_dered_r','airmass_i','skew','kurtosis','diff_g_i','sum_dered_u_dered_i',\n",
    "          'sum_r_dered_u','diff_redshift_dered_u','sum_z_redshift', 'mul_redshift_dered_i','mul_redshift_nObserve']:\n",
    "    cols.append(i)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_mlp=trn[cols].copy()\n",
    "cols.remove(\"class\")\n",
    "tst_mlp=tst[cols].copy()\n",
    "ftr=trn_mlp.drop(\"class\", axis=1)\n",
    "target=trn_mlp['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr=ftr.values\n",
    "tst_ar=tst_mlp.values\n",
    "target=target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(ftr)\n",
    "ftr = scaler.transform(ftr)\n",
    "tst_ar = scaler.transform(tst_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_p_val = np.zeros((ftr.shape[0], n_class))\n",
    "mlp_p_tst = np.zeros((tst_ar.shape[0], n_class))\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(ftr, target), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    mlp_clf = MLPClassifier(hidden_layer_sizes=(30,30,10),\n",
    "                            max_iter=10000,\n",
    "                            learning_rate_init=0.001,\n",
    "                            activation='relu',\n",
    "                            verbose=10,\n",
    "                            n_iter_no_change=20,\n",
    "                            random_state=seed\n",
    "                           )\n",
    "    mlp_clf.fit(ftr[i_trn], target[i_trn])\n",
    "    \n",
    "    mlp_p_val[i_val, :] = mlp_clf.predict_proba(ftr[i_val])\n",
    "    mlp_p_tst += mlp_clf.predict_proba(tst_ar) / n_fold\n",
    "print(f'{accuracy_score(target, np.argmax(mlp_p_val, axis=1)) * 100:.4f}%')\n",
    "print(confusion_matrix(target, np.argmax(mlp_p_val, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_dir = Path('C:/Users/ATIV/python/stacking/tst')\n",
    "val_dir = Path('C:/Users/ATIV/python/stacking/val')\n",
    "\n",
    "algo_name = 'mlp'\n",
    "feature_name = '41_scale'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "p_val_file = val_dir / f'{model_name}.val.csv'\n",
    "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
    "\n",
    "np.savetxt(p_val_file, mlp_p_val, fmt='%.6f', delimiter=',')\n",
    "np.savetxt(p_tst_file, mlp_p_tst, fmt='%.6f', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(ftr)\n",
    "ftr_scale = scaler.transform(ftr)\n",
    "tst_ar_scale = scaler.transform(tst_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_p_val = np.zeros((ftr_scale.shape[0], n_class))\n",
    "nb_p_tst = np.zeros((tst_ar_scale.shape[0], n_class))\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(ftr_scale, target), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    nb_clf = GaussianNB()\n",
    "    nb_clf.fit(ftr_scale[i_trn], target[i_trn])\n",
    "    \n",
    "    nb_p_val[i_val, :] = nb_clf.predict_proba(ftr_scale[i_val])\n",
    "    nb_p_tst += nb_clf.predict_proba(tst_ar_scale) / n_fold\n",
    "print(f'{accuracy_score(target, np.argmax(nb_p_val, axis=1)) * 100:.4f}%')\n",
    "print(confusion_matrix(target, np.argmax(nb_p_val, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_dir = Path('C:/Users/ATIV/python/stacking/tst')\n",
    "val_dir = Path('C:/Users/ATIV/python/stacking/val')\n",
    "\n",
    "algo_name = 'nb'\n",
    "feature_name = '124_scale'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "p_val_file = val_dir / f'{model_name}.val.csv'\n",
    "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
    "\n",
    "np.savetxt(p_val_file, nb_p_val, fmt='%.6f', delimiter=',')\n",
    "np.savetxt(p_tst_file, nb_p_tst, fmt='%.6f', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5번째 필터링\n",
    "clist5 = ['diff_u_i','mul_dered_i_dered_z','div_redshift_dered_r','mul_u_redshift']\n",
    "corr = trn.loc[trn['class']!=0,:].corr()\n",
    "\n",
    "cor_list=[]\n",
    "\n",
    "for i in range(0,trn.shape[1]):\n",
    "    for j in range(0,trn.shape[1]):\n",
    "        if abs(corr.iloc[i,j])>=0.9 and corr.index[i]!=corr.columns[j]:\n",
    "            cor_list += [corr.columns[j]]\n",
    "            cor_list += [corr.index[i]]\n",
    "\n",
    "groupby=cor_list\n",
    "cor_result=dict()\n",
    "for ip in tqdm(cor_list):\n",
    "    cor_result[ip]=cor_list.count(ip)\n",
    "print(cor_result)\n",
    "\n",
    "cols = list(trn.columns[~trn.columns.isin(list(cor_result.keys()))])\n",
    "\n",
    "for i in clist5:\n",
    "    cols.append(i)\n",
    "\n",
    "print(len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_svm=trn[cols].copy()\n",
    "cols.remove(\"class\")\n",
    "tst_svm=tst[cols].copy()\n",
    "ftr=trn_svm.drop(\"class\", axis=1)\n",
    "target=trn_svm['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr=ftr.values\n",
    "tst_ar=tst_svm.values\n",
    "target=target.values\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(ftr)\n",
    "ftr = scaler.transform(ftr)\n",
    "tst_ar = scaler.transform(tst_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_p_val = np.zeros((ftr.shape[0], n_class))\n",
    "svm_p_tst = np.zeros((tst_ar.shape[0], n_class))\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(ftr, target), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    estimator = SVC(verbose=True)\n",
    "    svm_clf = BaggingClassifier(base_estimator=estimator,\n",
    "                          n_estimators=10,\n",
    "                          n_jobs=-1)\n",
    "    svm_clf.fit(ftr[i_trn], target[i_trn])\n",
    "    \n",
    "    svm_p_val[i_val, :] = svm_clf.predict_proba(ftr[i_val])\n",
    "    svm_p_tst += svm_clf.predict_proba(tst_ar) / n_fold\n",
    "print(f'{accuracy_score(target, np.argmax(svm_p_val, axis=1)) * 100:.4f}%')\n",
    "print(confusion_matrix(target, np.argmax(svm_p_val, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_dir = Path('C:/Users/ATIV/python/stacking/tst')\n",
    "val_dir = Path('C:/Users/ATIV/python/stacking/val')\n",
    "\n",
    "algo_name = 'svm'\n",
    "feature_name = '124'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "p_val_file = val_dir / f'{model_name}.val.csv'\n",
    "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
    "\n",
    "np.savetxt(p_val_file, svm_p_val, fmt='%.6f', delimiter=',')\n",
    "np.savetxt(p_tst_file, svm_p_tst, fmt='%.6f', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Hist GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr=trn.drop(\"class\",axis=1)\n",
    "target=trn['class']\n",
    "ftr=ftr.values\n",
    "tst_ar=tst.values\n",
    "target=target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb_p_val = np.zeros((ftr.shape[0], n_class))\n",
    "hgb_p_tst = np.zeros((tst_ar.shape[0], n_class))\n",
    "# tree_model = DecisionTreeClassifier(max_depth = 10)\n",
    "\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(ftr, target), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    hgb_clf = HistGradientBoostingClassifier(max_iter=2000,\n",
    "                                      # validation_fraction=0.1,\n",
    "                                      # n_iter_no_change=15,\n",
    "                                      verbose=True,\n",
    "                                      random_state=42,\n",
    "                                          learning_rate = 0.01\n",
    "                                        )\n",
    "    \n",
    "    hgb_clf.fit(ftr[i_trn], target[i_trn])\n",
    "    hgb_p_val[i_val, :] = hgb_clf.predict_proba(ftr[i_val])\n",
    "    hgb_p_tst += hgb_clf.predict_proba(tst_ar) / 5\n",
    "    \n",
    "    y_pred = hgb_clf.predict(ftr[i_val])\n",
    "    y_pred = pd.Series(y_pred)\n",
    "    print(accuracy_score(pd.Series(target[i_val]), y_pred))\n",
    "    print()\n",
    "print(f'{accuracy_score(target, np.argmax(hgb_p_val, axis=1)) * 100:.4f}%')\n",
    "print(confusion_matrix(target, np.argmax(hgb_p_val, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_dir = Path('C:/Users/ATIV/python/stacking/tst')\n",
    "val_dir = Path('C:/Users/ATIV/python/stacking/val')\n",
    "\n",
    "algo_name = 'hgb'\n",
    "feature_name = '124'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "p_val_file = val_dir / f'{model_name}.val.csv'\n",
    "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
    "\n",
    "np.savetxt(p_val_file, hgb_p_val, fmt='%.6f', delimiter=',')\n",
    "np.savetxt(p_tst_file, hgb_p_tst, fmt='%.6f', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Cat Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr=trn.drop(\"class\",axis=1)\n",
    "target=trn['class']\n",
    "ftr=ftr.values\n",
    "tst_ar=tst.values\n",
    "target=target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_p_val = np.zeros((ftr.shape[0], n_class))\n",
    "cb_p_tst = np.zeros((tst_ar.shape[0], n_class))\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(ftr, target), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    cb_clf = CatBoostClassifier(iterations=10000,\n",
    "                               loss_function='MultiClass',\n",
    "                                random_seed = seed,\n",
    "                                task_type = \"GPU\" , \n",
    "                                eval_metric='Accuracy')\n",
    "    cb_clf.fit(ftr[i_trn], target[i_trn],\n",
    "            eval_set=[(ftr[i_val], target[i_val])])\n",
    "    \n",
    "    cb_p_val[i_val, :] = cb_clf.predict_proba(ftr[i_val])\n",
    "    cb_p_tst += cb_clf.predict_proba(tst_ar) / n_fold\n",
    "print(f'{accuracy_score(target, np.argmax(cb_p_val, axis=1)) * 100:.4f}%')\n",
    "print(confusion_matrix(target, np.argmax(cb_p_val, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_dir = Path('C:/Users/ATIV/python/stacking/tst')\n",
    "val_dir = Path('C:/Users/ATIV/python/stacking/val')\n",
    "\n",
    "algo_name = 'cb'\n",
    "feature_name = '124'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "p_val_file = val_dir / f'{model_name}.val.csv'\n",
    "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
    "\n",
    "np.savetxt(p_val_file, cb_p_val, fmt='%.6f', delimiter=',')\n",
    "np.savetxt(p_tst_file, cb_p_tst, fmt='%.6f', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Xtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr=trn.drop(\"class\",axis=1)\n",
    "target=trn['class']\n",
    "ftr=ftr.values\n",
    "tst_ar=tst.values\n",
    "target=target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "ext_p_val = np.zeros((ftr.shape[0], n_class))\n",
    "ext_p_tst = np.zeros((tst_ar.shape[0], n_class))\n",
    "# tree_model = DecisionTreeClassifier(max_depth = 10)\n",
    "\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(ftr, target), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    ext_clf = ExtraTreesClassifier(n_estimators=500, random_state=42)\n",
    "    \n",
    "    ext_clf.fit(ftr[i_trn], target[i_trn])\n",
    "    ext_p_val[i_val, :] = ext_clf.predict_proba(ftr[i_val])\n",
    "    ext_p_tst += ext_clf.predict_proba(tst_ar) / 5\n",
    "    \n",
    "    y_pred = ext_clf.predict(ftr[i_val])\n",
    "    y_pred = pd.Series(y_pred)\n",
    "    print(accuracy_score(pd.Series(target[i_val]), y_pred))\n",
    "    print()\n",
    "print(f'{accuracy_score(target, np.argmax(ext_p_val, axis=1)) * 100:.4f}%')\n",
    "print(confusion_matrix(target, np.argmax(ext_p_val, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_dir = Path('C:/Users/ATIV/python/stacking/tst')\n",
    "val_dir = Path('C:/Users/ATIV/python/stacking/val')\n",
    "\n",
    "algo_name = 'ext'\n",
    "feature_name = '124'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "p_val_file = val_dir / f'{model_name}.val.csv'\n",
    "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
    "\n",
    "np.savetxt(p_val_file, ext_p_val, fmt='%.6f', delimiter=',')\n",
    "np.savetxt(p_tst_file, ext_p_tst, fmt='%.6f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
